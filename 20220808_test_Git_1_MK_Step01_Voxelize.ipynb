{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d54e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import h5py\n",
    "import datetime as dt\n",
    "from jupyterlab_h5web import H5Web\n",
    "# import modules with the functionalities offered by CompositionSpace\n",
    "from compositionspace.get_gitrepo_commit import get_repo_last_commit\n",
    "from compositionspace.preparation import ProcessPreparation\n",
    "from compositionspace.segmentation import ProcessSegmentation\n",
    "from compositionspace.clustering import ProcessClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06e08a0-2049-440e-ac2e-c2c70f5277fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceil_to_multiple(number, multiple):\n",
    "    return multiple * np.ceil(number / multiple)\n",
    "\n",
    "def floor_to_multiple(number, multiple):\n",
    "    return multiple * np.floor(number / multiple)\n",
    "\n",
    "# ceil to a multiple of 1.5\n",
    "# print(ceil_to_multiple(23.0000000000000000000000000000000000000, 1.5))\n",
    "# floor to a multiple of 1.5\n",
    "# print(floor_to_multiple(-23.0000000000000000000000000000000000000, 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc5a514-9e53-492c-8aab-0af54ba23bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing compositionspace in the following working directory: /home/kaiobach/Research/hu_hu_hu/sprint19/compspace/CompositionSpace\n"
     ]
    }
   ],
   "source": [
    "# ! pip list  \n",
    "MY_PROCESSED_DATA_PATH = f\"{os.getcwd()}\"\n",
    "print(f\"Executing compositionspace in the following working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d069f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = False\n",
    "if run is True:\n",
    "    data = ProcessPreparation(f\"{MY_PROCESSED_DATA_PATH}/experiment_params.yaml\")\n",
    "    df_lst, files, ions, rrngs= data.get_apt_dataframe()\n",
    "    # print(f\"{df_lst}, type {type(df_lst)}\")\n",
    "    # print(files)\n",
    "    # print(ions)\n",
    "    # print(rrngs)\n",
    "\n",
    "    for idx, file in enumerate(files):\n",
    "        org_file = df_lst[idx]\n",
    "        atoms_spec = []\n",
    "        c = np.unique(rrngs.comp.values)\n",
    "        for i in range(len(c)):\n",
    "            range_element = rrngs[rrngs['comp']=='{}'.format(c[i])]\n",
    "            total, count = data.atom_filter(org_file, range_element)\n",
    "            name = i\n",
    "            total[\"spec\"] = [name for j in range(len(total))]\n",
    "            atoms_spec.append(total)\n",
    "    # print(atoms_spec)\n",
    "    df_atom_spec = pd.concat(atoms_spec)\n",
    "    # print(df_atom_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff17529-0e00-4a6b-aa12-45bf4eaa7f42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Remove code in this cell obsolete thanks to paraprobe NeXus</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22629265-7afc-48e7-93f4-dce993626ecd",
   "metadata": {},
   "source": [
    "## Load positions and iontypes from paraprobe-transcoder and paraprobe-ranger results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd3a7e2-6c3a-470c-8c64-e7fdad7e44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECONSTRUCTION_AND_RANGING = (\"R21_08680-v02.pos\", \"R21_08680.rrng\")\n",
    "RECONSTRUCTION_AND_RANGING = (\"PARAPROBE.Transcoder.Results.SimID.636502001.nxs\",\n",
    "                              \"PARAPROBE.Ranger.Results.SimID.636502001.nxs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d615c926-17f4-4ba2-93d9-acfe0230cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H5Web(RECONSTRUCTION_AND_RANGING[0])\n",
    "# H5Web(RECONSTRUCTION_AND_RANGING[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4cfaab3-72ab-4dfa-92d4-6d6349a533fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (4868202, 3), type <class 'numpy.ndarray'>, dtype float32\n",
      "73\n",
      "ion0, ('unknown iontype', 0)\n",
      "ion1, ('C ++', 1)\n",
      "ion2, ('C +', 2)\n",
      "ion3, ('O +', 3)\n",
      "ion4, ('Ti ++', 4)\n",
      "ion5, ('Ti ++', 5)\n",
      "ion6, ('Ti ++', 6)\n",
      "ion7, ('Ti ++', 7)\n",
      "ion8, ('Fe ++', 8)\n",
      "ion9, ('Fe +', 9)\n",
      "ion10, ('Fe ++', 10)\n",
      "ion11, ('Fe ++', 11)\n",
      "ion12, ('Fe ++', 12)\n",
      "ion13, ('Fe +', 13)\n",
      "ion14, ('Al ++', 14)\n",
      "ion15, ('Al +++', 15)\n",
      "ion16, ('Si ++', 16)\n",
      "ion17, ('Si ++', 17)\n",
      "ion18, ('Si ++', 18)\n",
      "ion19, ('Ti O ++', 19)\n",
      "ion20, ('Ti O ++', 20)\n",
      "ion21, ('Ti O ++', 21)\n",
      "ion22, ('Ti O ++', 22)\n",
      "ion23, ('Ti O ++', 23)\n",
      "ion24, ('Ti O +', 24)\n",
      "ion25, ('Ti O +', 25)\n",
      "ion26, ('Ti O +', 26)\n",
      "ion27, ('Ti O +', 27)\n",
      "ion28, ('Cr ++', 28)\n",
      "ion29, ('Cr ++', 29)\n",
      "ion30, ('Cr ++', 30)\n",
      "ion31, ('Cr +', 31)\n",
      "ion32, ('Cr +', 32)\n",
      "ion33, ('Mn ++', 33)\n",
      "ion34, ('Mn +', 34)\n",
      "ion35, ('Co ++', 35)\n",
      "ion36, ('Y +++', 36)\n",
      "ion37, ('Y ++', 37)\n",
      "ion38, ('Ga ++', 38)\n",
      "ion39, ('Ga +', 39)\n",
      "ion40, ('Cr O ++', 40)\n",
      "ion41, ('Cr O ++', 41)\n",
      "ion42, ('Cr O +', 42)\n",
      "ion43, ('Y O ++', 43)\n",
      "ion44, ('Fe H +', 44)\n",
      "ion45, ('Ni +', 45)\n",
      "ion46, ('Ni +', 46)\n",
      "ion47, ('Ni ++', 47)\n",
      "ion48, ('Ni +', 48)\n",
      "ion49, ('Ni ++', 49)\n",
      "ion50, ('V ++', 50)\n",
      "ion51, ('Mo ++', 51)\n",
      "ion52, ('Mo ++', 52)\n",
      "ion53, ('Mo ++', 53)\n",
      "ion54, ('Mo ++', 54)\n",
      "ion55, ('Mo ++', 55)\n",
      "ion56, ('Mo ++', 56)\n",
      "ion57, ('Mo', 57)\n",
      "ion58, ('B +', 58)\n",
      "ion59, ('B ++', 59)\n",
      "ion60, ('O H +', 60)\n",
      "ion61, ('Si O +', 61)\n",
      "ion62, ('Fe O +', 62)\n",
      "ion63, ('Fe O +', 63)\n",
      "ion64, ('Fe O +', 64)\n",
      "ion65, ('Ti O O +', 65)\n",
      "ion66, ('Al O ++', 66)\n",
      "ion67, ('Al O +', 67)\n",
      "ion68, ('Ti C +++', 68)\n",
      "ion69, ('Ti C +', 69)\n",
      "ion70, ('Ti C +', 70)\n",
      "ion71, ('O H H +', 71)\n",
      "ion72, ('As ++', 72)\n",
      "shape (4868202,), type <class 'numpy.ndarray'>, dtype uint8\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(RECONSTRUCTION_AND_RANGING[0], \"r\") as h5r:\n",
    "    trg = \"/entry1/atom_probe/reconstruction/reconstructed_positions\"\n",
    "    xyz = h5r[trg][:, :]\n",
    "    print(f\"shape {np.shape(xyz)}, type {type(xyz)}, dtype {xyz.dtype}\")\n",
    "\n",
    "    trg = \"/entry1/atom_probe/ranging/peak_identification\"\n",
    "    n_ion_types = len(h5r[trg])\n",
    "    iontypes = {}\n",
    "    for ion_id in np.arange(0, n_ion_types):\n",
    "        iontypes[f\"ion{ion_id}\"] = (h5r[f\"{trg}/ion{ion_id}/name\"][()].decode(\"utf8\"), np.uint8(ion_id))\n",
    "    print(f\"{n_ion_types}\")\n",
    "    for key, val in iontypes.items():\n",
    "        print(f\"{key}, {val}\")\n",
    "\n",
    "with h5py.File(RECONSTRUCTION_AND_RANGING[1], \"r\") as h5r:\n",
    "    trg = \"/entry1/iontypes/iontypes\"\n",
    "    ityp = h5r[trg][:]\n",
    "    print(f\"shape {np.shape(ityp)}, type {type(ityp)}, dtype {ityp.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8aa92-a3eb-4f43-b150-549de68af9e4",
   "metadata": {},
   "source": [
    "## Generate NXapm_composition_space results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380fefb3-0d0e-4d20-88d5-7a97c88d46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = f\"{MY_PROCESSED_DATA_PATH}/apm.composition.space.nxs\"\n",
    "with h5py.File(output_file_name, \"w\") as h5w:\n",
    "    h5w.attrs[\"@NX_class\"] = \"NXroot\"\n",
    "    h5w.attrs[\"@file_name\"] = f\"{MY_PROCESSED_DATA_PATH}/{output_file_name}\"\n",
    "    h5w.attrs[\"@file_time\"] = dt.datetime.now(dt.timezone.utc).isoformat()  # .replace(\"+00:00\", \"Z\")\n",
    "    # /@file_update_time\n",
    "    h5w.attrs[\"@NeXus_repository\"] = f\"https://github.com/FAIRmat-NFDI/nexus_definitions/blob/get_nexus_version_hash()\"\n",
    "    h5w.attrs[\"@NeXus_version\"] = f\"get_nexus_version()\"\n",
    "    h5w.attrs[\"@HDF5_version\"] = \".\".join(map(str, h5py.h5.get_libversion()))\n",
    "    h5w.attrs[\"@h5py_version\"] = h5py.__version__\n",
    "    \n",
    "    trg = \"/entry1\"\n",
    "    grp = h5w.create_group(trg)\n",
    "    grp.attrs[\"NX_class\"] = \"NXentry\"\n",
    "    dst = h5w.create_dataset(f\"{trg}/definition\", data=\"NXapm_compositionspace_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16838ba0-b08a-4e7f-be67-206343fab3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x-hdf5": "/home/kaiobach/Research/hu_hu_hu/sprint19/compspace/CompositionSpace/apm.composition.space.nxs",
      "text/plain": [
       "<jupyterlab_h5web.widget.H5Web object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H5Web(output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094e7cf-8fa7-4043-ada4-5ddff3051b05",
   "metadata": {},
   "source": [
    "## Import parameter via yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8149e9d-c639-4691-9376-6c1784ab7dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_path': 'tests/data', 'output_path': 'tests/output', 'n_big_slices': 10, 'voxel_size': 2, 'bics_clusters': 10, 'n_phases': 2, 'ml_models': {'name': 'GaussianMixture', 'GaussianMixture': {'n_components': 2, 'max_iter': 100000, 'verbose': 0}, 'RandomForest': {'max_depth': 0, 'n_estimators': 0}, 'DBScan': {'eps': 3, 'min_samples': 5}}}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "config_file_name = f\"{MY_PROCESSED_DATA_PATH}/experiment_params.yaml\"\n",
    "with open(config_file_name, \"r\", encoding=\"utf-8\") as yml:\n",
    "    config = yaml.safe_load(yml)\n",
    "    print(config)\n",
    "    print(config[\"voxel_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2fbfa-4c55-4cc0-b2c2-31af79e72a8b",
   "metadata": {},
   "source": [
    "## Voxelize with rectangular transfer function without creating slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33255c0b-04a8-4b84-9d70-c2ce3a62cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (4868202,)\n",
      "aabb3d [-32.  32.], extent 32\n",
      "[-30. -28. -26. -24. -22. -20. -18. -16. -14. -12. -10.  -8.  -6.  -4.\n",
      "  -2.   0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.\n",
      "  26.  28.  30.  32.]\n",
      "aabb3d [-32.  32.], extent 32\n",
      "[-30. -28. -26. -24. -22. -20. -18. -16. -14. -12. -10.  -8.  -6.  -4.\n",
      "  -2.   0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.\n",
      "  26.  28.  30.  32.]\n",
      "aabb3d [ 0. 72.], extent 36\n",
      "[ 2.  4.  6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34. 36.\n",
      " 38. 40. 42. 44. 46. 48. 50. 52. 54. 56. 58. 60. 62. 64. 66. 68. 70. 72.]\n",
      "[3213 3397 1717 1448  555  398 3303 1608 1780 3707]\n",
      "36847\n"
     ]
    }
   ],
   "source": [
    "# print(type(df_lst))\n",
    "column_names = ['x', 'y', 'z']\n",
    "# initialize extent (number of cells) along x, y, z axes\n",
    "extent = [0, 0, 0]\n",
    "# initialize min, max bounds for x, y, z\n",
    "aabb3d = np.reshape([np.finfo(np.float32).max, np.finfo(np.float32).min,\n",
    "          np.finfo(np.float32).max, np.finfo(np.float32).min,\n",
    "          np.finfo(np.float32).max, np.finfo(np.float32).min], (3, 2), order=\"C\")\n",
    "# print(aabb3d)\n",
    "n_ions = np.shape(xyz)[0]\n",
    "voxel_identifier = np.asarray(np.zeros(n_ions), np.uint32)\n",
    "print(f\"shape {np.shape(voxel_identifier)}\")\n",
    "# edge length of cubic cells/voxels in nm\n",
    "dedge = config[\"voxel_size\"]\n",
    "for axis_id in [0, 1, 2]:\n",
    "    column_name = column_names[axis_id]\n",
    "    # i = np.asarray(df_lst[0].loc[:, column_name], np.float32)\n",
    "    aabb3d[axis_id, 0] = floor_to_multiple(np.min((aabb3d[axis_id, 0], np.min(xyz[:, axis_id]))), dedge)\n",
    "    aabb3d[axis_id, 1] = ceil_to_multiple(np.max((aabb3d[axis_id, 1], np.max(xyz[:, axis_id]))), dedge)\n",
    "    extent[axis_id] = np.uint32((aabb3d[axis_id, 1] - aabb3d[axis_id, 0]) / dedge)\n",
    "    print(f\"aabb3d {aabb3d[axis_id, :]}, extent {extent[axis_id]}\")\n",
    "    bins = np.linspace(aabb3d[axis_id, 0] + dedge, aabb3d[axis_id, 0] + (extent[axis_id] * dedge), num=extent[axis_id], endpoint=True)\n",
    "    print(bins)\n",
    "    if axis_id == 0:\n",
    "        voxel_identifier = voxel_identifier + (np.asarray(np.digitize(xyz[:, axis_id], bins, right=True), np.uint32) * 1)\n",
    "    elif axis_id == 1:\n",
    "        voxel_identifier = voxel_identifier + (np.asarray(np.digitize(xyz[:, axis_id], bins, right=True), np.uint32) * np.uint32(extent[0]))\n",
    "    else:\n",
    "        voxel_identifier = voxel_identifier + (np.asarray(np.digitize(xyz[:, axis_id], bins, right=True), np.uint32) * np.uint32(extent[0]) * np.uint32(extent[1]))\n",
    "print(voxel_identifier[0:10])\n",
    "print(np.max(voxel_identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f61b04e1-9b2f-4631-b0ba-b212038d7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sorted lookup table for all ion positions (implicitly specified by their evaporation_id) O(n*log(n)) time-complexity operation wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f31fe6de-1bbe-455f-8634-7c9a63e6a64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( 8, 3213,  1) ( 8, 3397,  2) ( 8, 1717,  3) ( 8, 1448,  4)\n",
      " ( 8,  555,  5) ( 8,  398,  6) (29, 3303,  7) ( 8, 1608,  8)\n",
      " ( 8, 1780,  9) (29, 3707, 10)]\n",
      "[(0, 302,  2486) (0, 304,  3226) (0, 304,  5573) (0, 304, 13215)\n",
      " (0, 304, 17990) (0, 304, 18548) (0, 332,   173) (0, 333, 25249)\n",
      " (0, 334, 13610) (0, 334, 17234)]\n",
      "[(72, 30824, 4001574) (72, 32290, 4331875) (72, 32443, 4375588)\n",
      " (72, 32540, 4116453) (72, 32911, 4739678) (72, 33091, 4520185)\n",
      " (72, 33866, 4518381) (72, 34491, 4730446) (72, 34555, 4758833)\n",
      " (72, 34967, 4856367)]\n"
     ]
    }
   ],
   "source": [
    "ion_struct = [('iontype', np.uint8), ('voxel_id', np.uint32), ('evap_id', np.uint32)]\n",
    "lu_ityp_voxel_id_evap_id = np.zeros(n_ions, dtype=ion_struct)\n",
    "lu_ityp_voxel_id_evap_id[\"iontype\"] = ityp[:]\n",
    "del ityp\n",
    "lu_ityp_voxel_id_evap_id[\"voxel_id\"] = voxel_identifier\n",
    "# del voxel_identifier\n",
    "lu_ityp_voxel_id_evap_id[\"evap_id\"] = np.asarray(np.linspace(1, n_ions, num=n_ions, endpoint=True), np.uint32)\n",
    "print(lu_ityp_voxel_id_evap_id[0:10])\n",
    "lu_ityp_voxel_id_evap_id = np.sort(lu_ityp_voxel_id_evap_id, kind=\"stable\", order=[\"iontype\", \"voxel_id\", \"evap_id\"])\n",
    "print(lu_ityp_voxel_id_evap_id[0:10])\n",
    "print(lu_ityp_voxel_id_evap_id[-10::])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e839ac6-c719-4504-9400-80e51d278c04",
   "metadata": {},
   "source": [
    "Report results of voxelization and metadata for the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9efae095-f859-4ae9-9a64-fa11f08979db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxelization\n",
    "with h5py.File(output_file_name, \"a\") as h5w:\n",
    "    trg = \"/entry1/voxelization\"\n",
    "    grp = h5w.create_group(trg)\n",
    "    grp.attrs[\"NX_class\"] = \"NXprocess\"\n",
    "    dst = h5w.create_dataset(f\"{trg}/sequence_index\", data=np.uint64(1))\n",
    "\n",
    "    trg = \"/entry1/voxelization/cg_grid\"\n",
    "    grp = h5w.create_group(trg)\n",
    "    grp.attrs[\"NX_class\"] = \"NXcg_grid\"\n",
    "    dst = h5w.create_dataset(f\"{trg}/dimensionality\", data=np.uint64(3))\n",
    "    c = np.prod(extent)\n",
    "    dst = h5w.create_dataset(f\"{trg}/cardinality\", data=np.uint64(c))\n",
    "    dst = h5w.create_dataset(f\"{trg}/origin\", data=np.asarray([aabb3d[0, 0], aabb3d[1, 0], aabb3d[2, 0]], np.float64))\n",
    "    dst.attrs[\"units\"] = \"nm\"\n",
    "    dst = h5w.create_dataset(f\"{trg}/symmetry\", data=\"cubic\")\n",
    "    dst = h5w.create_dataset(f\"{trg}/cell_dimensions\", data=np.asarray([dedge, dedge, dedge], np.float64))\n",
    "    dst.attrs[\"units\"] = \"nm\"\n",
    "    dst = h5w.create_dataset(f\"{trg}/extent\", data=np.asarray(extent, np.uint32))  # max. 2*32 cells\n",
    "    identifier_offset = 0\n",
    "    dst = h5w.create_dataset(f\"{trg}/identifier_offset\", data=np.uint64(identifier_offset))  # start counting cells from 0\n",
    "\n",
    "    voxel_id = identifier_offset\n",
    "    position = np.zeros([c, 3], np.float64)\n",
    "    for k in np.arange(0, extent[2]):\n",
    "        z = aabb3d[2, 0] + (0.5 + k) * dedge\n",
    "        for j in np.arange(0, extent[1]):\n",
    "            y = aabb3d[1, 0] + (0.5 + j) * dedge\n",
    "            for i in np.arange(0, extent[0]):\n",
    "                x = aabb3d[0, 0] + (0.5 + i) * dedge\n",
    "                position[voxel_id, :] = [x, y, z]\n",
    "                voxel_id += 1\n",
    "    dst = h5w.create_dataset(f\"{trg}/position\", compression=\"gzip\", compression_opts=1, data=position)\n",
    "    dst.attrs[\"units\"] = \"nm\"\n",
    "    del position\n",
    "\n",
    "    voxel_id = identifier_offset\n",
    "    coordinate = np.zeros([c, 3], np.uint32)\n",
    "    for k in np.arange(0, extent[2]):\n",
    "        for j in np.arange(0, extent[1]):\n",
    "            for i in np.arange(0, extent[0]):\n",
    "                coordinate[voxel_id, :] = [i, j, k]\n",
    "                voxel_id += 1\n",
    "    dst = h5w.create_dataset(f\"{trg}/coordinate\", compression=\"gzip\", compression_opts=1, data=coordinate)\n",
    "    del coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "625f4da7-e0d4-4d37-9792-ad5fed59bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H5Web(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc4f2d-bf0a-47e7-b9e9-29ecc63d49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From this lookup we can very easily now compute the composition table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa22d512-1f4d-42d0-ba4e-4375b406578f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinality 36864\n",
      "means we have to visit so that many entries in the lookup table 238176\n",
      "but by virtue of construction of the lookup table all the indices will be close in cache\n",
      "ityp 0, np.sum(total_weights) 238176.0\n",
      "ityp 1, np.sum(total_weights) 238286.0\n",
      "ityp 2, np.sum(total_weights) 238571.0\n",
      "ityp 3, np.sum(total_weights) 241498.0\n",
      "ityp 4, np.sum(total_weights) 243540.0\n",
      "ityp 5, np.sum(total_weights) 245455.0\n",
      "ityp 6, np.sum(total_weights) 264818.0\n",
      "ityp 7, np.sum(total_weights) 266245.0\n",
      "ityp 8, np.sum(total_weights) 3816576.0\n",
      "ityp 9, np.sum(total_weights) 3818363.0\n",
      "ityp 10, np.sum(total_weights) 4060876.0\n",
      "ityp 11, np.sum(total_weights) 4143065.0\n",
      "ityp 12, np.sum(total_weights) 4157279.0\n",
      "ityp 13, np.sum(total_weights) 4157480.0\n",
      "ityp 14, np.sum(total_weights) 4159051.0\n",
      "ityp 15, np.sum(total_weights) 4159215.0\n",
      "ityp 16, np.sum(total_weights) 4162639.0\n",
      "ityp 17, np.sum(total_weights) 4162859.0\n",
      "ityp 18, np.sum(total_weights) 4163017.0\n",
      "ityp 19, np.sum(total_weights) 4164222.0\n",
      "ityp 20, np.sum(total_weights) 4171861.0\n",
      "ityp 21, np.sum(total_weights) 4172661.0\n",
      "ityp 22, np.sum(total_weights) 4173162.0\n",
      "ityp 23, np.sum(total_weights) 4173819.0\n",
      "ityp 24, np.sum(total_weights) 4175859.0\n",
      "ityp 25, np.sum(total_weights) 4176394.0\n",
      "ityp 26, np.sum(total_weights) 4176728.0\n",
      "ityp 27, np.sum(total_weights) 4177079.0\n",
      "ityp 28, np.sum(total_weights) 4207092.0\n",
      "ityp 29, np.sum(total_weights) 4771164.0\n",
      "ityp 30, np.sum(total_weights) 4834561.0\n",
      "ityp 31, np.sum(total_weights) 4834797.0\n",
      "ityp 32, np.sum(total_weights) 4835057.0\n",
      "ityp 33, np.sum(total_weights) 4837142.0\n",
      "ityp 34, np.sum(total_weights) 4837338.0\n",
      "ityp 35, np.sum(total_weights) 4838552.0\n",
      "ityp 36, np.sum(total_weights) 4842570.0\n",
      "ityp 37, np.sum(total_weights) 4842982.0\n",
      "ityp 38, np.sum(total_weights) 4843202.0\n",
      "ityp 39, np.sum(total_weights) 4843728.0\n",
      "ityp 40, np.sum(total_weights) 4844471.0\n",
      "ityp 41, np.sum(total_weights) 4844574.0\n",
      "ityp 42, np.sum(total_weights) 4844877.0\n",
      "ityp 43, np.sum(total_weights) 4847695.0\n",
      "ityp 44, np.sum(total_weights) 4849863.0\n",
      "ityp 45, np.sum(total_weights) 4850756.0\n",
      "ityp 46, np.sum(total_weights) 4851123.0\n",
      "ityp 47, np.sum(total_weights) 4852538.0\n",
      "ityp 48, np.sum(total_weights) 4852796.0\n",
      "ityp 49, np.sum(total_weights) 4853120.0\n",
      "ityp 50, np.sum(total_weights) 4854500.0\n",
      "ityp 51, np.sum(total_weights) 4855579.0\n",
      "ityp 52, np.sum(total_weights) 4856231.0\n",
      "ityp 53, np.sum(total_weights) 4857370.0\n",
      "ityp 54, np.sum(total_weights) 4858608.0\n",
      "ityp 55, np.sum(total_weights) 4859347.0\n",
      "ityp 56, np.sum(total_weights) 4861204.0\n",
      "ityp 57, np.sum(total_weights) 4861918.0\n",
      "ityp 58, np.sum(total_weights) 4862149.0\n",
      "ityp 59, np.sum(total_weights) 4862299.0\n",
      "ityp 60, np.sum(total_weights) 4864827.0\n",
      "ityp 61, np.sum(total_weights) 4865049.0\n",
      "ityp 62, np.sum(total_weights) 4865141.0\n",
      "ityp 63, np.sum(total_weights) 4865893.0\n",
      "ityp 64, np.sum(total_weights) 4865992.0\n",
      "ityp 65, np.sum(total_weights) 4866559.0\n",
      "ityp 66, np.sum(total_weights) 4866788.0\n",
      "ityp 67, np.sum(total_weights) 4867158.0\n",
      "ityp 68, np.sum(total_weights) 4867263.0\n",
      "ityp 69, np.sum(total_weights) 4867411.0\n",
      "ityp 70, np.sum(total_weights) 4867529.0\n",
      "ityp 71, np.sum(total_weights) 4868055.0\n",
      "ityp 72, np.sum(total_weights) 4868202.0\n",
      "cardinality of cg_grid 36864, n_ions 4868202\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(output_file_name, \"a\") as h5w:\n",
    "    trg = \"/entry1/voxelization/cg_grid\"\n",
    "    dst = h5w.create_dataset(f\"{trg}/voxel_identifier\", compression=\"gzip\", compression_opts=1, data=voxel_identifier)\n",
    "\n",
    "    c = np.prod(extent)\n",
    "    print(f\"cardinality {c}\")\n",
    "    # now just add weight/counts for a the iontype-specific part of the lookup-table\n",
    "    print(f\"means we have to visit so that many entries in the lookup table {np.sum(lu_ityp_voxel_id_evap_id['iontype'] == 0)}\")\n",
    "    print(f\"but by virtue of construction of the lookup table all the indices will be close in cache\")\n",
    "    total_weights = np.zeros(c, np.float64)\n",
    "    for ityp in np.arange(0, n_ion_types):\n",
    "        inds = np.argwhere(lu_ityp_voxel_id_evap_id[\"iontype\"] == ityp)\n",
    "        offsets = (np.min(inds), np.max(inds))\n",
    "        # print(f\"offsets {offsets}\")\n",
    "        # these are inclusive [min, max] array indices to use on lu_ityp_voxel_id_evap_id !\n",
    "        \n",
    "    # alternatively one could make two loops where in the first an offset lookup table is generated\n",
    "    # after this point one can drop the iontype and evap_id columns from the lu_ityp_voxel_id_evap_id lookup table\n",
    "        ityp_weights = np.zeros(c, np.float64)\n",
    "        for offset in np.arange(offsets[0], offsets[1] + 1):\n",
    "            idx = lu_ityp_voxel_id_evap_id[\"voxel_id\"][offset]\n",
    "            ityp_weights[idx] += 1.\n",
    "        # print(f\"ityp {ityp}, np.sum(ityp_weights) {np.sum(ityp_weights)}\")\n",
    "        \n",
    "        # atom/molecular ion-type-specific contribution/intensity/count in each voxel/cell\n",
    "        trg = f\"/entry1/voxelization/ion{ityp}\"\n",
    "        grp = h5w.create_group(f\"{trg}\")\n",
    "        grp.attrs[\"NX_class\"] = \"NXion\"\n",
    "        dst = h5w.create_dataset(f\"{trg}/name\", data=iontypes[f\"ion{ityp}\"][0])\n",
    "        dst = h5w.create_dataset(f\"{trg}/weight\", compression=\"gzip\", compression_opts=1, data=ityp_weights)\n",
    "        dst.attrs[\"units\"] = \"a.u.\"  \n",
    "        \n",
    "        total_weights += ityp_weights\n",
    "        print(f\"ityp {ityp}, np.sum(total_weights) {np.sum(total_weights)}\")\n",
    "    print(f\"cardinality of cg_grid {c}, n_ions {n_ions}\")\n",
    "\n",
    "    # total atom/molecular ion contribution/intensity/count in each voxel/cell\n",
    "    trg = f\"/entry1/voxelization\"\n",
    "    dst = h5w.create_dataset(f\"{trg}/total\", compression=\"gzip\", compression_opts=1, data=total_weights)\n",
    "    dst.attrs[\"units\"] = \"a.u.\"\n",
    "\n",
    "# reload weights and compute to compositions if really needed as compositions\n",
    "# dst = h5w.create_dataset(f\"{trg}/composition\", compression=\"gzip\", compression_opts=1, data=###)\n",
    "# dst.attrs[\"unit\"] = \"a.u.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a74215-494e-4c6e-8953-b914d3ec4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a large number of voxels, say a few million and dozens of iontypes storing all\n",
    "# ityp_weights in main memory might not be useful, instead these should be stored in the HDF5 file\n",
    "# inside the loop and ones the loop is completed, i.e. each total weight for each voxel known\n",
    "# we should update the data in the HDF5 file, alternatively one could also just store the\n",
    "# weights instead of the compositions and then compute the composition with a linear in c*ityp time\n",
    "# complex division, there are even more optimizations one could do, but probably using\n",
    "# multithreading would be a good start before dwelling deeper already this code here is\n",
    "# faster than the original one despite the fact that it works on the entire portland wang\n",
    "# dataset with 4.868 mio ions, while the original test dataset includes only 1.75 mio ions\n",
    "# the top part of the dataset also the code is much shorter to read and eventually even\n",
    "# more robust wrt to how ions are binned with the rectangular transfer function\n",
    "# one should say that this particular implementation (like the original) one needs\n",
    "# substantial modification when one considers a delocalization kernel which spreads\n",
    "# the weight of an ion into the neighboring voxels, this is what paraprobe-nanochem does\n",
    "# one can easily imagine though that the results of this voxelization step can both be\n",
    "# fed into the composition clustering step and here is then also the clear connection\n",
    "# where the capabilities for e.g. the APAV open-source Python library end and Alaukik's\n",
    "# ML/AI work really shines, in fact until now all code including the slicing could have\n",
    "# equally been achieved with paraprobe-nanochem.\n",
    "# Also the excessive reimplementation of file format I/O functions in datautils should\n",
    "# be removed. There is an own Python library for just doing that more robustly and\n",
    "# capable of handling all sorts of molecular ions and charge state analyses included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0546df9b-c182-4082-84ae-a3e2e8390a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x-hdf5": "/home/kaiobach/Research/hu_hu_hu/sprint19/compspace/CompositionSpace/apm.composition.space.nxs",
      "text/plain": [
       "<jupyterlab_h5web.widget.H5Web object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H5Web(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2dd04-905d-4c47-a8a1-8b95fc0f7963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
